---
title: '将人工智能生成的代码视为草稿'
titleJp: ''
date: '2025-12-02'
excerpt: ''
tags: ["Translate"]
---
简而言之： 将人工智能生成的代码视为草稿。它可以编写初稿，但绝不能让其他人审阅。 没有人工审核，就无法可靠地追溯行为背后的意图。一旦停止审核人工智能草稿，你就无法了解代码运行的根本原因。实际上，应该对人工智能编写的代码采用与人类团队成员编写的代码相同的标准。

# 永远不要外包阅读工作——务必亲自 review 人工智能的初稿。

人工智能可以编写代码的初稿，但必须由人类进行阅读和审查，以确保代码的意图和质量。

果停止审查人工智能生成的代码草稿，你就无法了解代码运行的原因（或者它是否真的有效）——因为无法可靠地追溯行为与意图。换句话说， 低级模型（LLM）本身不会发布糟糕的代码，是团队会发布 。当没有人负责检查人工智能编写的代码时，糟糕的代码就会漏网，这并非因为模型本身失败，而是因为工作流程未能要求更高的标准 [1] 。

将人工智能的输出视为不可信的输入 ——它可能语法正确，甚至通过了测试，但在经过人工验证之前，它并不值得信任。人工智能模型经常生成看似合理但存在细微缺陷的代码 ，包括虚构的函数或不安全的模式 [2] 。因此，切勿合并未经人工阅读和理解的代码。正如一位工程师所说，盲目信任未经验证的人工智能输出会立即导致错误， 并且 “系统性地降低我们发现这些错误的能力”，因为验证代码所需的技能会因长期不用而退化 [3] 。

简而言之，要始终坚持人机协作：人工智能可以编写代码，但只有人才能确保代码的行为符合预期目的。

# 盲目依赖人工智能会削弱批判性思维和技能。

工程领导者担心，盲目接受人工智能生成代码的开发人员会丧失批判性思维能力。

这种担忧并非空穴来风——早期研究已经证实了这一点。研究发现，过度使用人工智能助手与大脑参与度降低和批判性思维能力下降相关 [4] 。实际上，依赖人工智能的开发人员可能会忽略一些基本任务，例如阅读文档或自行调试错误。一位资深工程师坦言，使用人工智能的即时答案反而让他“技艺下降”。

他不再阅读文档（“既然 LLM 能立刻解释清楚，何必费劲呢？”），甚至不再分析错误——相反，他会将堆栈跟踪复制粘贴到 AI 中，然后再将 AI 的答案粘贴回代码。“我简直成了个人形剪贴板，”他抱怨道 [5] 。这种认知卸载意味着开发者不再需要推理问题；AI 负责思考，而人只是负责转录。其结果不仅导致技能下降，也降低了警惕性——如果开发者认为 AI 总是正确的，他们可能会错过以前能够发现的细微错误或安全问题。事实上，AI 输出的便捷性和完美性会让工程师产生一种虚假的安全感，降低他们在代码审查中的怀疑精神 [6] 。

讽刺的是，人工智能原本旨在提升生产力，但过度依赖反而会降低个人能力。正如一位作者所言：“我们并没有因为人工智能而成为能力提升十倍的开发者，而是对人工智能的依赖程度提高了十倍。”——我们为了追求短期速度而牺牲了长期理解 [7] 。由此可见，要保持工程思维的敏锐度，你必须始终对代码保持深入的思考。将人工智能视为工具，而非拐杖——始终质疑并验证其解决方案，而不是盲目接受。

# 跳过学习过程而追求速度会阻碍成长。

许多团队为了追求速度，直接投入到人工智能的使用中，却忽略了使用人工智能时应该伴随的学习和理解过程。

人工智能编码工具的优势在于其高速生成能力—— 生成、生成、再生成 ——但这往往是以开发者无法真正理解他们所构建的东西为代价的。当你依赖人工智能编写自己并不完全理解的代码时，你就跳过了让你成为更优秀工程师的关键学习过程 [8] 。

传统编程过程中伴随的错误、试错和研究并非仅仅是障碍，它们更是培养关键技能的训练场。如果将繁重的工作外包给人工智能，尤其是初级开发人员，可能永远无法获得评估或改进所生成代码所需的深度知识。

这就形成了一个恶性循环： 你因为缺乏经验而使用人工智能，导致代码质量差；而你又因为不断使用人工智能而永远无法获得经验 [8] 。正如一位评论员直言不讳地问道， 如果你的角色仅仅沦为提示人工智能编写你不理解的代码，那么你又能创造什么价值呢？ [9 ]

我们基本上跳过了人工智能可以作为学习辅助工具或辅导老师的阶段，直接将其用作自动输出代码的工具。理想情况下，开发者应该利用人工智能来加深理解 ——例如，让人工智能解释一段复杂的代码，或者解释某个解决方案的原理——甚至在将代码交给其他人之前，先用人工智能进行一次本地的“自我审查”。但实际上，许多人只是简单地点击“接受”建议就继续前进。这意味着他们或许能更快地交付功能，但对功能的工作原理或某些模式的运用原因却知之甚少。

随着时间的推移，这种理解上的不足会逐渐积累成严重的技能差距。资深工程师担心，一些新手虽然能在人工智能的辅助下快速编写代码，却难以调试或扩展代码，因为他们从未学习过底层概念。事实上，工程主管们反映，虽然初级工程师现在发布功能的速度比以往任何时候都快，但一旦出现问题，“他们就很难调试自己不理解的代码” [10] 。

软件工程的精髓远不止于编写可运行的代码 ——它更在于理解代码的编写原理 ，以及如何对其进行改进和演进。如果我们绕过这一过程，就有可能培养出一代只能依赖人工智能自动运行的程序员。为了避免这种情况，我们应该将人工智能的输出视为学习的机会：不要只是简单地复制粘贴答案，而是要认真阅读、提出问题，并确保自己能够向同事解释清楚。利用人工智能来加速你的工作，而不是阻碍你作为工程师的成长 [11] [12] 。

我听说过一些资深开发者退回的 PR，明明是用 AI 生成的，但提交者却不理解 AI 的原理。当初级开发者提交 AI 生成的 PR 时，评审就成了指导的主要场所。要提出苏格拉底式的问题，引导他们解释 AI 的输出结果。这能确保他们理解，而不仅仅是功能。评审的重点应该放在理解上，而不仅仅是正确性。

# 代码审查在人工智能生成的代码面前举步维艰。

传统的代码审查方法难以应对人工智能生成的代码，导致团队不确定如何保持代码质量。

代码审查一直是发现错误和确保代码质量的安全保障。但人工智能的出现改变了这一切：人工智能可以瞬间生成更大的差异 ，通常会涉及多行或多个文件，这意味着审查人员在每个拉取请求中都要面对更大的工作量和潜在的更复杂的情况。事实上，研究发现，包含大量 Copilot 生成代码的拉取请求平均审查时间会延长约 26% ，因为审查人员必须理清不熟悉的代码模式，并仔细检查是否存在人工智能特有的错误 [13] 。

审阅者还报告了一种心理效应：在审查自己未编写的代码时，尤其是在语法精良的情况下，他们的信心会下降——他们需要更长时间来验证逻辑，并且可能会怀疑自己的理解 [14] 。人工智能可以生成看起来简洁现代（命名一致、格式规范）的代码，这会降低审阅者的怀疑态度 [6] 。如果代码“看起来很专业”，就很容易认为它是可靠的，这使得一些细微的错误或设计缺陷更容易被忽略。

另一个复杂之处在于意图的丢失 。在传统的代码审查中，审查者可以讨论“作者的意图”——存在一个可以与实现进行比较的人类意图。而对于人工智能生成的代码，代码作者可能无法完全理解每一行代码背后的意图，因为他们并非以传统的方式编写代码。最初提供给人工智能的提示本质上就是规范，但审查者通常看不到这个提示 [15] 。这意味着审查者只能猜测需求以及人工智能的解决方案是否真正满足这些需求，而不仅仅审查代码是否有效。

正如一份报告指出的那样， 审查人员不再评估开发者的意图，而是评估模型实际执行的操作 [16] 。传统的代码审查清单（侧重于风格、明显的逻辑错误等）已经不够用了，因为人工智能代码可能会以非传统的方式失败——例如，使用初级开发人员不知道的过时算法，或者引入不易察觉的极端情况错误。

团队也面临着代码审查过载的问题 。人工智能结对程序员生成代码的速度比人类更快，这意味着单个开发人员可以在短时间内提交大量代码请求。这种“速度”可能会超出团队进行全面审查的能力。这就像代码中的“垃圾”——大量的输出淹没了审查人员，以至于难以准确定位问题 [17] 。在这种情况下，一些组织制定了新的策略：例如，如果一个代码请求中超过 30% 的代码（按行数或内容计算）是由人工智能生成的，则可能会触发额外的审查流程或由更高级别的审查人员进行审查 [18] 。

其理念在于承认大量使用人工智能的代码需要不同程度的审查，而不是 business-as-usual.。另一种新兴的做法是标记人工智能贡献：在拉取请求或提交消息中明确标记“此代码由人工智能辅助编写”。这可以提醒审查人员格外警惕。事实上，专家建议对人工智能生成的代码进行标记和跟踪， 以确保责任追溯——这有助于审查人员了解需要查找的内容，并帮助团队日后追踪错误（“这个错误是否来自人工智能编写的代码？”） [19] 。

然而，公开标注人工智能的使用会带来文化上的挑战：开发者必须感到心理安全才能披露人工智能的使用情况。如果人们担心使用人工智能会受到评判（“我的团队会不会认为我懒惰或能力不足？”），他们可能会选择隐瞒——这对团队来说更糟糕。隐瞒人工智能的使用意味着团队无法了解潜在风险所在，也无法据此调整评审策略 [20] 。为了应对这种情况，具有前瞻性的团队会鼓励透明化，并消除偏见。

使用人工智能应该像使用任何工具一样——使用它没问题，但你必须对输出结果负责。正如某指南所说， 永远不要把错误或质量问题归咎于人工智能 ；提交代码的工程师对此负有责任， 就这么简单 [21] 。如果每个人都接受这种理念，那么说“我使用 Cursor 来辅助编写这个模块”就仅仅是陈述事实，而不是承认过错。这使得团队能够共同确保人工智能生成的代码部分得到应有的重视。

目前，我们的代码审查工具和规范仍无法满足这些需求。我们还没有广泛应用自动检测 PR 中 AI 代码的工具，而且大多数差异查看器也无法显示 AI 的提示或推理过程。因此，我们需要依靠流程和团队协议来弥补这一不足——明确指出 AI 编写的代码，更严格地审查测试，并可能对没有断点的 AI 代码的大小设定限制，以便人工审查。

如果有问题的代码未经质疑就通过了 PR，那么问题不仅仅在于人工智能，还在于代码审查流程不够完善，无法发现这些问题 [22] 。这呼吁我们采取行动，代码审查实践必须随着人工智能的普及而发展。

Stack Overflow 的一项调查显示 ，66% 的开发者表示，他们对 AI 助手最常见的不满是代码“几乎正确，但还不够完美”。45% 的开发者表示，调试 AI 生成的代码是他们最大的时间黑洞。质量门和验证如今已成为关键路径。

# 最佳实践：将 AI 生成的代码视为草稿

要有效利用人工智能编码工具，我们必须调整习惯和流程。不妨将人工智能的输出视为初级开发人员的初稿 ——虽然有价值，但需要仔细审查和完善。以下是一些切实可行的最佳实践，可确保人工智能生成的代码在提高生产力的同时， 不会牺牲质量或理解力：

永远不要 Merge 你不理解的代码。 如果某些代码是由人工智能生成的，那么作为开发者的你有责任逐行阅读并确保理解其含义。你应该能够解释代码的功能和原因。如果人工智能生成的代码片段中有任何你无法理解的部分，请将其视为危险信号——要么改进提示，要么让人工智能解释，要么自己重写该部分。一些开源项目明确要求贡献者证明他们理解自己提交的代码 ，即使代码是由人工智能编写的。在专业环境中，同样的原则也适用：对你提交的任何代码负全部责任，无论其作者是谁（或由什么程序编写） [21] 。在实践中，这意味着在代码提交到团队代码库之前，你需要运行代码、编写或审查测试，并逐步推演其逻辑 。

永远不要合并你不理解的代码。 如果某些代码是由人工智能生成的，那么作为开发者的你有责任逐行阅读并确保理解其含义。你应该能够解释代码的功能和原因。如果人工智能生成的代码片段中有任何你无法理解的部分，请将其视为危险信号——要么改进提示，要么让人工智能解释，要么自己重写该部分。一些开源项目明确要求贡献者证明他们理解自己提交的代码 ，即使代码是由人工智能编写的。在专业环境中，同样的原则也适用：对你提交的任何代码负全部责任，无论其作者是谁（或由什么程序编写） [21] 。在实践中，这意味着在代码提交到团队代码库之前，你需要运行代码、编写或审查测试，并逐步推演其逻辑 。

将 AI 用作编码助手，而非代码编写者——将其融入你的思考过程。 不要只是让 AI 生成代码然后盲目粘贴，而是以对话式、解释性的方式使用它。例如，你可以让 AI 解释它刚刚建议的代码，或者为其生成注释。你可以让它为代码建议测试用例，然后运行这些用例来验证代码是否真的有效。AI 还可以通过总结大型差异或识别 PR 中的潜在问题区域来提供帮助（一些高级代码审查工具现在提供 AI 生成的摘要）。所有这些用途都让你——人——始终掌握主动权。你是在利用 AI 来增强你 ​​ 的理解，而不是取代它。一个推荐的做法是， 首先审查 AI 生成的更改的测试 [26] ——确保有一个完善的测试套件覆盖新代码。如果测试薄弱或缺失，那就表明你需要编写更多测试，然后再信任代码。此外，务必对 AI 代码进行严格的代码检查和静态分析：AI 可能不会默认遵循团队的惯用编程风格，因此需要使用自动化工具强制执行风格和架构规则 [27] [28] 。如果 AI 提出的建议不符合你常用的模式，请毫不犹豫地进行重构。本质上，让 AI 成为你的结对程序员 ，负责编写代码草稿和提出想法，但最终的修改和决策仍然由你来做。

对人工智能生成的代码进行全面测试并确保其安全性。 对人工智能编写的代码应用与手工编写代码相同（或更高）的测试级别至关重要。编写单元测试和集成测试以覆盖所有功能。尤其要注意边界情况和潜在的故障模式——人工智能常常会处理“正常路径”，但忽略异常输入或错误处理。此外，还要考虑安全性：如果人工智能借鉴了存在缺陷的代码示例，则可能会引入常见的漏洞，例如 SQL 注入、XSS、不安全的反序列化等 [29] [30] 。使用安全代码检查工具或扫描器（例如 Semgrep 或 Bandit 等工具可以发现明显的漏洞 [31] ）。如果人工智能生成了任何依赖项或配置，请务必检查其中是否存在敏感信息或不安全的默认设置。对待人工智能的代码，就像对待你雇佣的、你并不完全信任的承包商一样——仔细检查所有内容，因为最终你的团队要对任何错误或安全漏洞负责 ，无论代码是谁编写的。

在寻求同行评审之前，利用人工智能进行自我审查。 一个高效的做法是，在提交拉取请求之前， 先让人工智能对其输出进行评价 。例如，在收到代码建议后，你可以询问：“你觉得这段代码有哪些潜在问题？有哪些特殊情况或改进空间？”人工智能可能会指出你没有考虑到的情况，或者更符合惯用语的写法。这就像逻辑的拼写检查——并非万无一失，但它可以发现一些显而易见的问题。这并不能取代人工评审，但它可以帮助你清理代码草稿， 避免让同行被显而易见的问题分散注意力。你可以把它想象成你与人工智能合作润色代码，然后再将其交给团队。这也有助于你学习，因为人工智能的评审意见可以突出显示你需要考虑的领域。但请记住，要验证人工智能的任何反馈；有时它可能会“臆想”出一些并不存在的问题，所以要运用你的判断力。

在寻求同行评审之前，利用人工智能进行自我审查。 一个高效的做法是，在提交拉取请求之前， 先让人工智能对其输出进行评价 。例如，在收到代码建议后，你可以询问：“你觉得这段代码有哪些潜在问题？有哪些特殊情况或改进空间？”人工智能可能会指出你没有考虑到的情况，或者更符合惯用语的写法。这就像逻辑的拼写检查——并非万无一失，但它可以发现一些显而易见的问题。这并不能取代人工评审，但它可以帮助你清理代码草稿， 避免让同行被显而易见的问题分散注意力。你可以把它想象成你与人工智能合作润色代码，然后再将其交给团队。这也有助于你学习，因为人工智能的评审意见可以突出显示你需要考虑的领域。但请记住，要验证人工智能的任何反馈；有时它可能会“臆想”出一些并不存在的问题，所以要运用你的判断力。

在与团队共享代码时，务必记录并标注 AI 的贡献。 在拉取请求描述或代码注释中，注明哪些部分是由 AI 生成的，或者您是否在解决方案中大量依赖 AI，会很有帮助。例如：“使用 Gemini/Opus/GPT 生成了此排序算法的初始实现；审查并修改了结果。”这种透明度有助于审查人员了解重点所在。这并非为了指责 AI 或您，而是为了提供上下文 。事实上，我们鼓励使用清晰的注释或注解来标记 AI 生成的代码，以此来建立问责制和可追溯性 [33] 。如果之后出现奇怪的 bug，团队可以追溯到源头，发现“哦，这部分代码是根据提示 X 由 AI 编写的”，这可能会使调试更容易。当然，这需要在支持性的文化氛围中进行（参见下一节）——目标是共同维护质量，而不是指责某人。一些团队甚至会保留 AI 辅助更改的日志以用于审计 [33] 。至少，您可以考虑与审阅者分享您使用的提示信息，例如在 PR 评论中。这样，审阅者就能理解您的要求 ，并判断 AI 的代码是否真正符合您的意图 [15] 。这种将提示信息作为规范的方法可以弥合意图与实现之间的差距。

总之，将 AI 代码视为草稿意味着要像对待人类新手编写的代码一样，对其进行严格的审查 ：深入审查、全面测试，并且在验证之前不要想当然地认为任何内容是正确的。AI 可以大幅加快样板代码的编写速度，甚至可以提出解决方案，但你是工程师 ——你必须负责任地将这些建议整合到代码库中。

# 制定人工智能生成代码的团队协议

为了成功地将人工智能融入开发流程，团队应该制定清晰的指导方针——本质上就是一份“合同”——来规范如何处理人工智能生成的代码。 这是一个全新的领域，任何不一致都可能导致摩擦或质量问题。团队工作协议可以包含围绕人工智能使用的规则、职责和文化规范。以下是一些团队正在采用的关键要素：

确保问责机制不失效。 明确规定，将人工智能生成的代码集成到代码库中的人员对此负全部责任，不容置疑。不得将责任推卸给人工智能。如果引入了错误，其处理方式与其他任何引入的错误相同。行业指南支持这一原则，即开发人员必须对其提交的任何代码承担全部责任，无论代码是谁编写的，并且必须像测试自己的代码一样彻底测试人工智能生成的代码 [21] 。管理层应强调，使用人工智能并非降低代码质量的借口。代码审查员和审批人也承担责任——如果您批准了一项变更，您就如同往常一样为其担保。本质上，人工智能并不会改变“代码所有者”的定义。

明确人工智能的使用方式和时机。 团队应讨论哪些类型的任务适合使用人工智能辅助。例如，大家可能一致认为人工智能非常适合生成单元测试、样板代码、搭建框架或探索多种方法——但或许会避免在没有额外审查的情况下将其用于核心复杂算法。有些团队可能禁止将人工智能用于安全敏感代码或关键算法，除非有高级工程师密切监督。而另一些团队则可能认为，只要遵守其他规则（理解、测试等），人工智能可以用于任何用途。关键在于设定预期。这还涉及到伦理和法律方面的考量 （例如，确保人工智能的输出不包含复制的许可代码，或不会引入偏见），但这本身就是一个值得探讨的话题。重点在于，达成共识的政策可以避免误解，例如某个开发人员合并了其他开发人员不认可的大量人工智能编写的代码块。

强调透明度和心理安全。 团队合同应鼓励开发人员公开人工智能的参与情况。例如，可以制定一条准则：“如果人工智能在变更中发挥了重要作用，请在 PR 中提及。” 领导者必须营造一种氛围，让这种承认被视为积极（尽职调查），而不是消极。缺乏透明度会导致代码库中出现“影子人工智能”——即由人工智能编写但无人知晓的代码，这会增加调试和维护的难度 [20] 。为了避免这种情况，应将透明度作为常态。一种做法是在代码中添加简单的注释，例如“// 代码由人工智能辅助生成”，或者在 PR 中使用标签。团队还可以约定在项目 wiki 或代码审查中记录提示信息，以供将来参考 [33] 。如果有人觉得自己对人工智能生成的代码部分没有完全理解，他们应该可以放心地提出疑问，并寻求帮助或额外的审查 [33] 。与其假装一切正常，不如坦诚地承认“我对 Copilot 生成的代码并非完全有信心”。心理安全感能确保人们畅所欲言，最终保护代码质量和开发人员的成长。

将人工智能意识融入代码审查流程。 团队应更新代码审查清单或完成定义，以涵盖人工智能。例如，审查清单可以添加诸如“如果代码是由人工智能生成的，作者是否提供了提示或描述了意图？”或“对于人工智能生成的代码，请仔细检查常见问题（边界情况、安全性、风格一致性）”之类的条目。一些组织通过要求对大量使用人工智能的代码进行额外审查来正式落实这一点，如前文所述 [34] 。培训课程也很有帮助——团队可以举办一次关于“典型的人工智能错误”的非正式会议，以便所有审查人员都知道需要注意哪些方面（例如不必要的复杂性、缺少空值检查等）。团队还可以采用一些辅助工具，例如人工智能驱动的代码分析工具，它可以标记出可能存在问题的代码模式。最终，整个审查文化可能会发生转变，更加严格地对待人工智能的贡献。作为一条共同规则，您可以这样说： 任何人工智能生成的代码都必须经过彻底的人工审查才能合并，没有任何例外 。这似乎显而易见，但明确指出这一点，就表明了速度不会凌驾于质量之上。

支持持续学习和技能发展。 为了解决批判性思维衰退的问题，团队协议可以明确鼓励保持技能敏锐的实践。例如，结对编程，其中一人不使用人工智能，并解释其思考过程；或者轮流负责不使用人工智能的复杂错误修复。甚至可以鼓励开发人员偶尔先“手动”实现一些功能，然后再使用人工智能进行优化。一些公司甚至追踪人工智能对调试时间的影响，并确保员工仍然知道如何在不使用工具的情况下进行故障排除 [10] 。协议可以是：“我们使用人工智能来加速日常任务，但我们仍然希望工程师能够理解并手动处理复杂的部分。”通过在团队原则中承认这一点，您可以肯定人类专业知识的重要性。尤其是主管和经理应该以身作则——在代码审查中表明，他们会像审查任何代码一样仔细审查人工智能生成的代码，并提出深思熟虑的问题。初级开发人员会从中汲取灵感，并明白人工智能并非免于思考的捷径。

本质上，团队的 AI 代码协议旨在维护质量、清晰度和信任 。每个人都应该了解 AI 的使用方式，并就其输出必须符合的标准达成一致。这份“协议”可以是一个动态文件，随着经验的积累而不断完善。其目标是防止 AI 悄然降低代码库或削弱工程师技能的情况发生。相反，有了规则，AI 就可以成为强大的加速器， 并设有安全保障 。它促使人们就以前隐含的话题（例如“你是否理解你提交的内容？”）展开讨论——现在，我们将其明确化。

# 结论：人工智能不能取代理解

人工智能编码工具已成为我们生活的一部分，它们尤其擅长生成代码草稿 ——无论是框架、样板代码，甚至是人类从零开始编写可能需要花费更长时间的复杂代码。拥抱它们可以极大地提高生产力，并将开发人员从繁重的工作中解放出来。但是，一旦我们开始将人工智能生成的代码视为“一劳永逸”的工具，我们就破坏了我们所追求的益处。

人工智能在软件工程中的真正价值在于将其速度与人类判断力相结合。 这意味着始终以批判的眼光审视人工智能的输出，保持对代码运行原理的好奇心，并坚持代码的清晰性和正确性。 将人工智能生成的代码视为草稿，意味着承认它仍在不断完善中——需要人类的洞察力进行润色和完善。

通过坚持高标准的代码质量和开发者培训，我们确保人工智能能够增强而非削弱我们的能力 。即使“结果”唾手可得，我们也始终关注“为什么”和“如何做”。简而言之：不要停止阅读代码。

无论代码是由实习生、人工智能还是经验丰富的同事编写的，只有能够理解代码才能值得信赖。如果你从不将阅读和思考工作外包，你就能始终将代码的行为与其背后的意图联系起来——这正是软件工程的精髓所在。

当然，利用人工智能可以加快速度，但双手一定要握住方向盘。

最终投入生产环境的代码必须始终经过人的审阅和审核。这样，我们才能兼得两者之长：既能享受人工智能生成的初稿的高效性，又能获得经过人工审核、易于理解的最终代码的可靠性。

> https://substack.com/home/post/p-179880341
